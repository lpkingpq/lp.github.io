# MIT 6.824 第一节 MapReduce学习

* 需要先阅读mapreduce的论文。

* MR是课程6.824主题的一个很好的例子，也是实验1的主要关注点。
 
* MapReduce 概要
	* 背景：几个小时处理完TB集基本的数据集，例如：实验分析爬行网页的结构，通常不是由分布式系统开发的爱好者开发的，这就会非常痛苦，比如如何处理错误。
	* 总体目标：非专业程序员可以轻松的在合理的效率下解决巨大的数据处理问题。程序员定义Map函数和Reduce函数，顺序代码一般都比较简单。MR在成千的机器上面运行处理大量的数据输入，隐藏全部分布式的细节。

* MapReduce的抽象试图是，输入会被分配到不同的分片，MR在每个分片调用Map()函数，产生中间数据集<k2, v2>,然后MR将会收集相同k2值的v2,然后将v2分别传输给Reduce()函数，最后的输出是数据集<k2,v3>
```
	input is divided into "splits"
	Input Map -> a,1 b,1 c,1
	Input Map ->     b,1
    Input Map -> a,1     c,1
                |   |   |
                    |   -> Reduce -> c,2
                    -----> Reduce -> b,2
```
* 例子：word count 输入时成千上万的文件
	```
	Map（k,v）
		split v into words
		for each word w
			emit(w, "1")

	Reduce(k,v)
		emit(len(v))
    ```

* 这个模式很容易编程，隐藏了很多让人痛苦的细节
	* 并发：顺序执行相同的结果
	* starting s/w on servers ??
	* 数据移动
	* 失败
	
* 这个模型容易扩展nx台计算机可以同时执行nx个map函数和reduce函数，map函数不需要等待或者共享数据，完全可以并行的执行。在一定程度上，你可以通过购买更多的计算机来获取更大的吞吐量。而不是每个应用程序专用的高效并行。电脑是比程序员更便宜。

* 影响性能的因素
	* 我们关心的就是我们需要优化的。cpu、内存、硬盘、网络。它们一般将会被网络限制，网络中的全部数据通常远小于主机网络链接速度。一般情况下很难建立一个比单机快1000倍的网络，所以关心尽量减少数据在网络传输。

* 容错如何处理
	* 比如：如果服务器在执行MR工作是崩溃怎么办？隐藏这个错误是非常困难的，为什么不重新执行这个工作呢？
	
	* MR重新执行失败的Map函数和Reduce函数，他们是纯函数---他们不会改变数据输入、不会保持状态、不共享内存、不存在map和map，或者reduce和reduce之间的联系。
	
	* 所以重新执行页会产生相同的输出。纯函数的这个需求是MR相对其他并行编程方案的主要限制，然后也是因为这个需求使得MR非常简单。
* 更多细节：master:给works分配工作，记得中间输出的位置。NaN.输入分割，输入存在GFS，每个分配拷贝三份，全部电脑运行GFS和MR works,输入的分片远远多于woker的数量，NaN。 master在每台机器上面执行Map任务，当原来的任务完成之后map会处理新的任务，worker讲输出key散列映射输出到R分区保存在本地磁盘上，Nan。当全部没有Map执行的时候Reduce将会执行。master告诉Reduce去获取Map works产生的中间数据分区，reduce work将最终的结果输出到GFS中。

* 有哪些详细的设计帮助提升了网络性能
	* Map的输入来自本地的硬盘而非网络
	* 中间数据只在网络上面传输一次，保存本地硬盘，而不是GFS
	* 中间数据通过key被划分到多个文件，“大网络传输”更加有效
	
* 负载均衡如何处理的
	* 扩展的关键：不同的大小，不同的内容和不同的服务器硬件配置导致处理分配或者分区的时间不是一致的。

	* 解决方案：分配的数据要比worker数量大。master不断的将分片分配给那些已经完成之前任务的work进行处理。所以没有分片是特别大的，分片的大小只影响完成的时间，同时速度更快的服务器将会处理更多的工作，最后一起完成。

* MR怎么应对work崩溃？
	* Map worker崩溃：
		* master重新执行，基于GFS的其他副本输入传输任务，即使work已经完成，因为master依然需要硬盘上的数据，有些reduce workers也许在读取中间数据的时候就已经失败，我们依赖于功能和确定性的函数。
	* master如何监控work崩溃 pings
	* 如果reduces已经获取全部的中间数据，那么master不需要重启map函数，如果reduce崩溃那么必须等待map再次运行。
	* reduce worker在输出结果前崩溃，master必须在其他worker上面重新开始任务。
	* reduce worker在输出结果的过程中崩溃，GFS会自动重命名输出，然后使其保持不可见，直到reduce完成。所以master在其他地方再次运行Reduce worker将会是安全的。

* 其他错误和问题
	* 假如master意外的开启两个map，worker处理同一个输入会怎么样？ 它只会告诉reduce worer其中的一个。
	* 假如咯昂个reduce worker 处理中间数据的同一个分区会怎么样？他们都会将同一份数据写到GFS上面，GFS的原子重命名操作会触发（系统自带的rename函数），先完成的获胜，并将结果写到GFS中。
	* 假如一个worker非常慢怎么办？----一个落伍者?产生原因可能是非常糟糕的硬件设施，当一个mapreduce操作接近完成的时候，master会调度备用任务进场来执行剩下的、处于处理中状态的的任务。无论是最初的执行进程、还是备用任务进场完成任务，我们都把这个任务标记为已完成。
	* 假如一个worker因为软件或者硬件导致计算结果错误怎么办的，MR建设是建立在fiail-stop的cpu和软件之上。
	* master崩溃怎么办？checkpoint机制了解下。
	
* 关于哪些MapReduce不能很好执行的应用？
	* 并不是所有工作都适合map/shuffle/reduce这种模式
	* 小的数据，因为管理成本太高，如非网站后端
	* 大数据中的小更小，比如添加一些文件到大的索引
	* 不可预知的读（map/reduce都不能选择输入）
	* 多数灵活的系统允许mr,但是使用非常复杂的模型


********************************************************

### 接下来这部分主要是对mapreduce论文的翻译的抄写，我坚信一句话：好记性不如烂笔头，在这里抄写部分论文内容。加深下理解。

# 1 实现

&#8195;MapReduce模型可以有多重不同的实现方式。如何正确选择取决于具体的环境.例如，选择一种实现方式是适用于小型的共享内存方式的机器。另外一种实现方式则适用于大型NUMA架构的多处理器的主机。而有的实现方式更适合大型的网络链接集群。

&#8195;本章描述了一个适用于谷歌内部广泛使用的运行环境的实现；用以太网交换机连接、由普通pc机器组成的大型集群。在我们的环境里包括：

* x86架构、运行Linux操作系统，双处理器、2-4GB内存的机器。
* 普通的王丽硬件设备，每个机器的带宽为百兆或者千兆，但是远小于网络的平均带宽的一半。
* 机器中包含成百上千的机器，因此，机器故障时常态。
* 存储为廉价的内置IDE硬盘。一个内部分布式文件系统来管理存储在这些磁盘上的数据。文件系统通过数据复制来在不可靠的硬件上保证数据的可靠性和有效性。
* 用户提交工作job给调度系统，每个工作job都包含一系列的任务task，调度系统将这些任务调度到集群中多台可用机器上。

# 1.1 执行概况

&#8195;通过将map调用的输入数据自动分割为m个数据片段的集合，map调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将map调用产生的中间key值分成R个不同的分区（例如：hash（key）mod R）。Reduce调用也被分布到多台机器上执行。分区数量R和分区函数由用户来指定。
![](https://i.imgur.com/1g1poDY.png)

&#8195;上图展示了我们MapReduce实现中操作的全部流程。当用户调用MapReduce函数时，将发生项目的一系列动作。

1、用户程序首先调用MapReduce库将输入文件分成M个数据片段，每个数据片段的大小一般从16MB到64MB（可以通过可选的参数来控制每个数据片段的大小）。然后用户程序在集群中穿件大量的程序副本fork。

2、这些程序副本中有一个特殊的程序-master。副本中其他的程序都是worker程序，由master分配任务。有m个Map任务和R个reduce任务将被分配，master将一个map任务或者reduce任务分配给一个空闲的worker。

3、被分配了map任务的work程序读取相关的输入数据片段，从输入的数据片段中解析出key/value pair,然后把key/value pair 传给用户自定义的Map函数，由map函数生成并输出的中间key/value pair, 并缓存在内存中。

4、缓存中key/value pair通过分区函数分成R个区域，之后周期性的写入到本地磁盘上。缓存kev/value pair在本地磁盘上的存储位置将被回传给master，由master负责把这些存储位置再传送给Reduce woker。

5、当Reduce worker程序接收到master程序发来的数据存储位置信息后，使用RPC从Map worker所有的主机的磁盘上读取这些缓存数据。当reduce worker读取了所有的中间数据后，通过对key进行排序，使得具有相同key值的数据聚合在一起。由于许多不同的key值会被映射到相同的Reduce任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就需要在外部进行排序。

6、Reduce worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce worker程序将这个key值和他相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。

7、当所有的map和reduce任务都完成之后，master唤醒用户程序。在这个时候，用户程序里的对mapReduce调用才返回。

&#8195;在成功完成任务之后，MapReduce的输出存放在R个输出文件中（对应每个Reduce任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这R个输出文件合并成一个文件---它们经常把这些文件作为另外一个mapReduce的输入，或者在将另外一个可以处理多个分割文件的分布式应用中使用。

# 1.2 Master 数据结构

&#8195; Master持有一些数据结构，它存储每一个map和reduce任务的状态（空闲、工作中或者完成），以及work机器（非空闲任务的机器）的标识。

&#8195; Master就像一个数据管道，中间文件存储区域的位置信息通过这个管道从Map传递到Reduce.因此，对于每个已经完成的map任务，master存储了map任务产生的R个中间文件存储区域的大小和位置。当map任务完成时，master接收到位置和大小的更新信息，这些信息将逐步递增给那些正在工作的reduce任务。

# 2 容错

&#8195; 因为MapReduce库的设计初衷是使用成百上千的机器组成的集群来处理超大规模的数据。所以，这个库必须要能很好的处理机器故障。

# 2.1 worker故障

&#8195;master 周期性的ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息。master将把这个worker标记为无效。所有由这个失效的worker完成的map任务将被重设为初始的空闲状态，之后这个任务就可以被安排给其他的worker。同样的，worker失效时正在运行的map或者reduce任务也将被重新置位空闲状态，等待重新调度。

&#8195;当worker故障时，由于已经完成的map任务的输出存储在这台机器上，map任务的输出已不可访问了，因此必须重新执行。**而已完成的Reduce任务的输出存储在全局文件系统上，因此不需要再执行**

&#8195;当一个Map任务首先被workerA执行，之后由于worker A失效了又被调度到worker B执行，**这个“重新执行”的动作会被通知给所有执行Reduce任务的worker。**任何还没有从worker读取数据的Reduce任务将worker B读取数据。

&#8195;MapReduce可以处理大规模worker失效的情况。比如，在一个MapReduce操作执行期间，在正在运行的集群上进行网络维护引起80台机器在几分钟内不可访问了，MapReduce master只需要简单的再次执行那些不可访问worker完成的工作。之后继续执行未完成的任务，知道最终完成这个MapReduce操作。

# 2.2 master失效

&#8195;一个简单的解决方法是让master周期性的将上面描述的数据结构写入磁盘，即checkpoint(检查点)机制。如果这个master任务失效了，可以从最后一个检查点开始启动另一个master进程。然而，由于只有一个master进程，master失效后再恢复是比较麻烦的，因此我们现在的实现是如果master失效，就终止MapReduce运算。客户可以减产这个状态，并且可以根据需要重新执行MapReduce操作。

# 2.3 在失效方面的处理机制

&#8195; 当用户提供map和reduce操作是输入确定性函数（即先沟通呢的输入产生相同的输出）时，我们的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。

&#8195; 我们一来对map和reduce任务的输出是**原子提交**的来完成这个特性。每个工作中的任务把它的输出写到私有的临时文件中。每个Reduce任务生产一个这样的文件，而每个Map任务则生成R个这样的文件（一个Reduce任务对应一个文件）。当一个Map任务完成时，worker发送一个包含R个临时文件名的完成消息给master。如果master从一个已经完成的Map任务再次接收到一个完成消息，master将忽略这个消息；否则，master将这R个文件的名字记录在数据结构里。

&#8195;当Reduce任务完成时，Reduce worker进程①**原子**的方式把临时文件重命名为最终的输出文件。如果一个Reduce任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。我们一来底层文件系统的重命名操作的原子性来保证最终文件系统状态仅仅包含一个Reduce任务产生的数据。

&#8195;使用MapReduce模型的程序员可以很容易的理解他们程序定行为，因为我们绝大多数的Map和Reduce操作是确定性的，而且存在这样一个事实：我们的失效处理机制等价于一个顺序的执行操作。当Map或者Reduce操作是不确定的时间。我们提供的机制虽然较弱的但是依然合理的处理机制。当使用非确定性操作的时候，一个Reduce任务R1的输出等价于一个非确定性程序顺序执行产生的时的输出。但是，另一个Reduce任务R2的输出也许符合一个不同的非确定顺序程序执行产生的R2的输出。

&#8195;考虑Map任务M和Reduce任务R1、R2的情况。我们设定e(Ri)是Ri已经提交的过程（有且仅有一个这样的执行过程）。当e(R1)读取了由M一次执行产生的输出，而e(R2)读取了由m的另一次执行产生的输出，导致了较弱的失效处理。**这段没看明白**

# 2.4 存储位置

&#8195; 在我们的计算运行环境中，网络带宽是一个相当匮乏的资源。我们通过尽量把输入数据（由GFS管理）存储在集群中机器的本地磁盘上来节省网络带宽。GFS把每个文件按64MB一个block分隔。每个block保持在多台机器上，环境中就存放多份拷贝，一般是3份。Mapreduce的master在调度Map任务时会考虑输入文件的位置信息，尽量将一个map任务调度在包含相关输入数据拷贝的机器上执行；如果上述独立失败了，master将阐释在保持有输入数据拷贝的机器附近的机器上执行map任务（例如，分配到一个和包含输入数据的机器在一个switch里的worker机器上执行）。当在一个足够大的cluster集群上运行大型MapReduce操作的时候，大部分的输入数据都能从本地读取，因此消耗非常少的网络带宽。

# 2.5 任务粒度

&#8195; 如前所述，我们把Map拆分成M个片段，把Reduce拆分成R个片段执行。理想情况下，M和R应当比集群中woker的机器的数量要多得多。在每台woker机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度；失效机器上执行的大量Map任务都可以分布到其他的work机器上去执行。

&#8195;但是实际上，在我们的具体实现中对M和R的取值有一定的客观的限制。因为master必须执行O(M+N)次调度，并且内存中保持了O(M*R)个状态（对影响内存使用的因素还是比较小的）；O(M*R)块状态，大概对Map任务/Reduce任务1个字节就可以了。

&#8195;更进一步，R值通常是由用户指定的。因为每个Reduce任务最终都会生成一个独立的输出文件。实际使用时我们也倾向选择合适的M值，以使得每一个独立任务都是处理大约16M到64M的输入数据（这样上面描写的输入数据存储优化策略才有效）。另外，**我们把R值设置为我们想使用的worker机器数的小倍数**。我们通常会用这样的比例来执行MapReduce：M=200000,R=5000，使用2000台work机器。

# 2.6 备用任务

&#8195; 影响一个MapReduce的总执行时间最通常的因素是“落伍者”：在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过了预期。出现“落伍者”的原因非常多。比如：如果一个机器的硬盘出了问题，在读取的时候要经常进行读取纠错操作。导致读取数据的速度从30M/s降低到1M/s。如果cluster的调度系统在这台机器上又调度了其他任务，由于CPU、内存、本地硬盘和网络带宽等竞争因素的存在，导致执行MapReduce代码的执行效率更加缓慢。

&#8195;我们有一个通用的机制来减少“落伍者”出现的情况。**当一个MapReduce操作接近完成的时候，**master调用备用backup进程来执行剩下的、处于中间中状态的任务。无论是最初的执行进程、还是备用任务进程完成了任务，我们都把这个任务标记成为已完成。我们调优了这个机制，通常只会比正常操作多几个百分点的计算机资源。我们发现采用这样的机制对于减少超大MapReduce操作的总处理时间效果显著。

# 3 技巧

&#8195; 虽然简单的Map和Reduce函数提供的基本功能已经能够满足大部分的计算需要，我们还是发掘出来一些有价值的扩展功能。本节将描述这些扩展功能。

# 3.1 分区函数

&#8195;MapReduce的使用者通常会指定Reduce任务和Reduce任务输出文件的数量R。我们在中间key上使用分区函数来对数据进行分区，之后再输入到后续任务执行进程。一个缺省的分区函数是使用hash方法（比如，hash(key)mod R）进行分区。hash方法能产生非常平衡的分区。然而，有时候，其他一些分区函数对key值进行的分区将非常有用。比如，输出key值是URLs。我们希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，mapreduce库的用户需要提供专门的分区函数。使用“hash(hostname(urlkey)) mode R”作为分区函数就可以把所有来自同一个主机的URLs保存在同一个输出文件中。

# 3.2 顺序保证

&#8195;我们确保在给定的分区中，中间key/value pair数据的处理顺序是按照key值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按key值随机存钱的应用非常有意义。对在排序输出的数据也很有帮助。

# 3.3 Combiner函数

&#8195;在某些情况下，Map函数产生的中间key值的重复数据会占很大的比例，并且，用户自定义的Reduce函数满足结合律和交换律。我们讲到的单词数统计程序是个很好的例子。由于词频倾向于一个zipf分布。每个Map任务将产生成千上万个这样的记录<the,1>。所有的这些记录将通过网络被发送到一个单独的Reduce任务，然后又这个Reduce任务把所有这些记录累加产生一个数字。我们允许用户指定一个可选的combiner函数，combiner函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。

&#8195;Combiner函数在每台执行Map任务的机器上都会被执行一次，一般情况下，Combiner和Reduce函数一样的。Combiner函数和Reduce函数之间唯一的区别是MapReduce库怎样控制函数的输出。Reduce函数的输出被保持在最终的输出文件里，而Combiner函数的输出被写到中间文件里，然后被发送给Reduce任务。

&#8195;部分的合并中间结果可以显著的提高一些MapReduce操作的速度。

# 3.4 输入和输出的类型

&#8195;MapReduce库支持几种不同的格式的输入数据。比如，文本模式的输入数据的每一行被视为是一个key/value pair。key是文件的偏移量，value是那一行的内容。另外一种常见的格式是以key进行排序来存储的key/value pair的序列。每种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独Map任务来进行后续处理（例如，文本模式的范围分割必须确保仅仅在每行的边界进行范围分割）。虽然大多数MapReduce的使用者仅仅使用很少的预定义输入类型就满足要求了，但是使用者依然可以通过提高一个简单的Reader接口实现就能够支持一个新的输入类型。

&#8195;Reader并非一定要从文件中读取数据，比如，我们可以很容易的实现一个从数据库里读记录的Reader或者从内存中的数据结构读取数据的Reader。

&#8195;类似的，我们提供了一些预定义的输出数据的类型。通过这些预定义类型能够产生不同格式的数据。用户采用类似添加新的输入数据类型的方式增加新的输出类型。

# 3.5 副作用

&#8195;在某些情况下，MapReduce的使用者发现，如果在Map和/或者Reduce操作过程中增加辅助的输出文件比较省事。我们依靠程序writer把这种"副作用"变成原子的和幂等的。通常应用程序输出首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作rename重命名这个临时文件。

&#8195;如果一个任务产生了多个输出文件，我们没有提供类似两阶段提交的原子操作支持这种情况。因此，对于会产生多个输出文件，并且对于跨文件有一致性要求的任务，都必须是确定性的任务。但是在实际中，这个限制还没有给我们带来过麻烦。

# 3.6 跳过损坏的记录

&#8195;有时候，用户程序中的bug导致Map或者Reduce函数在处理某些记录的时候crash掉了。MapReduce操作无法顺利完成。通常的做法是修复bug后再次执行MapReduce操作，但是，有时候找出这些bug并修复他们不是一件容易的事情；这些bug也许是在第三方库里面，而我们手头没有这些库的源代码。而在很多时候，忽略一些问题的记录也是可以接受的，比如在一个巨大的数据集上进行统计分析的时候。我们提供了一种知性模式，为了保证整个处理继续进行，MapReduce会监测那些记录导致确定性的crash，并且跳过这些记录不处理。

&#8195;每个worker进程都设置了信号函数补货内存异常和总线错误。在执行Map或者Reduce操作之前，MapReduce库通过全局变量保存记录序号。如果用户程序处罚了一个系统信号，消息处理函数将用“最后一口气”通过UDP包想master发送处理的最后一条记录的序号。当master看到处理某条特定记录不止失败一次时，master就标志着这条记录需要被跳过，并且在下次重新执行相关Map或者Reduce任务的时候跳过这条记录。

# 3.7 本地执行

&#8195;调试Map和Reduce函数的bug是非常困难的，因为实际执行操作时不但是分布在系统中的执行的，而且通常是好几千台计算机上执行的，具体的执行位置由master进行动态调度的，这又大大增加了调试难道。为了简化调试，profile和小规模测试，我们开发了一套MapReduce库本地实现版本，通过使用本地版本的MapReduce库，MapReduce操作在本地计算机上顺序的执行。用户可以控制MapReduce操作的执行，可以把操作限制在Map任务上，用户通过设定特别的标志来在本地执行它们的程序，之后就可以很容易的使用本地调试的测试工具例如gdb。

# 3.8 状态信息

&#8195;master使用嵌入式的Http服务器显示一组状态信息页面，用户可以监控各种执行状态。状态信息页面包括了执行的进度，比如已经完成了多少个任务，有多少个任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等。页面还包含了指向每个任务stderr和stdout文件的链接。用户根据这些数据预测计算需要执行大约多长时间，是否需要增加额外的计算资源。这些页面也可以来分析什么时候计算执行的比预期的要慢。

&#8195;另外最顶层的状态页面显示了哪些worker失效了，以及他们失效的时候正在运行的Map和Reduce任务，这些对于调试用户代码中的bug很有帮助。

# 3.9 计数器

&#8195; MapReduce库使用计数器统计不同事件发生的次数。比如，用户可能想统计已经处理了多少个单词，已经索引了多少篇German稳定等等。

&#8195;为了使用这个特性，用户在程序中创建一个命名的计数器对象，在Map和Reduce函数中相应的计数器的值，例如：


	Counter * uppercase;
	uppercase = GetCounter("uppercase");
	map(String name, string contents);
	for each word w in contents:
		if(IsCapitalized(w)):
			uppercase->Increment();
	EmitIntermediate(w,"1");
		

&#8195;这些计数器的值周期性的从各个单独的worker机器上传递给master（附加在ping的应答包中传递）。master把执行成功的Map和Reduce任务的计数器值进行累加。当MapReduce操作完成之后，返回给用户代码。

&#8195;计数器当前的值也会显示在master的状态页面上，这样用户就可以看到当前计算的进度。当累加计数的时候，master要检查重复运行的Map或者Reduce任务，避免重复累加（之前提到的备用任务和失效后重新执行任务这两种情况也会导致相同的任务被多次执行）。

&#8195;有些计数器的值是由MapReduce库自动维持的，比如已经处理的输入的key/value pair的数量、输出的key/value pair的数量等待。

&#8195;计数器机制对于MapReduce操作的完整性检查非常有用。比如，在某些MapReduce操作中，用户需要确保输出的key/value pair精确的等于输入key/value pair,或者处理German文档数量在处理的整个文档数量中属于合理范围。